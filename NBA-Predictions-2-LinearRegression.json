{"paragraphs":[{"title":"Imports","text":"import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport com.databricks.spark.csv\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\nimport scala.util.matching.Regex\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions.{udf,col,concat,lit,when,ceil}\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865193_2140433647","id":"20160627-021745_1849588760","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 11:34:02 AM","dateFinished":"Jul 14, 2016 11:34:04 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:907","errorMessage":""},{"title":"Use a custom schema to control types","text":"// This line reads in the file and parses it with a CSV reader\nval sqlContext = new SQLContext(sc)\n// For some reason, the reader below doesn't infer the types properly (all strings) .  Forcing the types here\nval customSchema = StructType(Array(\n    StructField(\"scorea\", IntegerType, true),\n    StructField(\"scoreb\", IntegerType, true),\n    StructField(\"timeleft\", DoubleType, true),\n    StructField(\"teama\", StringType, true),\n    StructField(\"teamb\", StringType, true),\n    StructField(\"scorea-scoreb\", IntegerType, true),\n    StructField(\"scoreb-scorea\", IntegerType, true),\n    StructField(\"pct-complete\", DoubleType, true),\n    StructField(\"pct-left\", DoubleType, true),\n    StructField(\"cf1\", DoubleType, true),\n    StructField(\"cf2\", DoubleType, true),\n    StructField(\"teamaspread\", DoubleType, true),\n    StructField(\"overunder\", DoubleType, true),\n    StructField(\"teambspread\", DoubleType, true),\n    StructField(\"teama_vegas_fscore\", DoubleType, true),\n    StructField(\"teamb_vegas_fscore\", DoubleType, true),\n    StructField(\"key\", StringType, true),\n    StructField(\"fscorea\", DoubleType, true),\n    StructField(\"fscoreb\", DoubleType, true),\n    StructField(\"fscorea-fscoreb\", IntegerType, true),\n    StructField(\"fscoreb-fscorea\", IntegerType, true),\n    StructField(\"away-win\", DoubleType, true),\n    StructField(\"home-win\", DoubleType, true),\n    StructField(\"teama_adj_fscore\", DoubleType, true),\n    StructField(\"teamb_adj_fscore\", DoubleType, true),\n    StructField(\"pfscoreb-pfscorea\", DoubleType, true)\n   \n))\n\n\n\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993890834_-526414221","id":"20160627-021810_1098980962","dateCreated":"Jun 27, 2016 2:18:10 AM","dateStarted":"Jul 14, 2016 11:34:08 AM","dateFinished":"Jul 14, 2016 11:34:08 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:908","errorMessage":""},{"title":"Read in CSV file for linear regression","text":"var linearDF = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .schema(customSchema)\n    .load(\"/resources/data/nba-datawrangle-lrDF.csv\") \n    ","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467040098186_-927268593","id":"20160627-150818_1570637254","dateCreated":"Jun 27, 2016 3:08:18 PM","dateStarted":"Jul 14, 2016 11:34:10 AM","dateFinished":"Jul 14, 2016 11:34:10 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:909","errorMessage":""},{"title":"Inspect the data","text":"linearDF.show(5)","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865193_2140433647","id":"20160627-021745_1056639877","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 11:34:12 AM","dateFinished":"Jul 14, 2016 11:34:15 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:910","errorMessage":""},{"title":"Create Splits for Training/CrossVal/Test","text":"// Shrink the data just for now ...\nval splits = linearDF.randomSplit(Array(0.70,0.30), seed = 11L)\nval trainingdf = splits(0).cache()\nval testdf = splits(1).cache()\n\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865196_2139279400","id":"20160627-021745_1027445704","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 11:34:20 AM","dateFinished":"Jul 14, 2016 11:34:20 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:911","errorMessage":""},{"title":"Build Linear Regression ML Pipeline - Predict  Home / Away team scores","text":"// Linear regression requires that\n// The best way I found to modify data in a custom way is with map, however it returns an RDD, so you will need to run toDF at the end\n// Logistic DF requires a DF of type  => [label: double, features: vector]\n\n\n\n// Configure an ML pipeline, which consists of two stages: assembler and linear regression\n\nval assembler_ab = new VectorAssembler()\n  .setInputCols(Array(\"scorea\", \"scoreb\", \"teama_adj_fscore\", \"teamb_adj_fscore\", \"timeleft\", \"overunder\", \"teama_vegas_fscore\", \"teamb_vegas_fscore\",\"teamaspread\" ))\n  .setOutputCol(\"features\")\n\n//val output = assembler_a.transform(linearDF)\n  \nval lr_a = new LinearRegression()\n  .setMaxIter(50)\n  .setRegParam(0.01)\n  .setLabelCol(\"fscorea\")\n  .setFeaturesCol(\"features\")\n\nval pipeline_a = new Pipeline()\n  .setStages(Array(assembler_ab, lr_a))\n\n// Fit the pipeline to training documents.\nval model_a = pipeline_a.fit(trainingdf)\nval predictions_a = model_a.transform(testdf)\n\nval lr_b = new LinearRegression()\n  .setMaxIter(50)\n  .setRegParam(0.01)\n  .setLabelCol(\"fscoreb\")\n  .setFeaturesCol(\"features\")\n\nval pipeline_b = new Pipeline()\n  .setStages(Array(assembler_ab, lr_b))\n\n// Fit the pipeline to training documents.\nval model_b = pipeline_b.fit(trainingdf)\nval predictions_b = model_b.transform(testdf)\n\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"time-left","index":0,"aggr":"sum"}],"values":[{"name":"team-a-differential","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"time-left","index":0,"aggr":"sum"},"yAxis":{"name":"team-a-differential","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865198_2140048898","id":"20160627-021745_485109493","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 11:34:36 AM","dateFinished":"Jul 14, 2016 11:35:06 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:912","errorMessage":""},{"text":"predictions_a.select('scorea,'timeleft,'prediction).show(5)\npredictions_b.select('scoreb,'timeleft,'prediction).show(5)","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467140200741_-1437792993","id":"20160628-185640_1379207392","dateCreated":"Jun 28, 2016 6:56:40 PM","dateStarted":"Jul 14, 2016 11:37:39 AM","dateFinished":"Jul 14, 2016 11:37:40 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:913","errorMessage":""},{"title":"Run Linear Regression without ML pipeline","text":"/////////////////////////////////////////////////////\n//Sans pipeline\n// Running this way, because I found it hard to access my intermediate \n// model results\n//\n// This was the easiest way for me to access model weight and query the summaries\n//\n\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval training_ab = assembler_ab.transform(trainingdf)\nval test_ab = assembler_ab.transform(testdf)\n\nval lr_xa = new LinearRegression()\n  .setMaxIter(50)\n  .setRegParam(0.01)\n  .setLabelCol(\"fscorea\")\n\n// Fit the pipeline to training documents.\nval model_xa = lr_xa.fit(training_ab)\nval predictions_xa = model_xa.transform(test_ab)\nval evaluator_xa = new RegressionEvaluator().setLabelCol(\"fscorea\")\n\nval lr_xb = new LinearRegression()\n  .setMaxIter(50)\n  .setRegParam(0.01)\n  .setLabelCol(\"fscoreb\")\n\n// Fit the pipeline to training documents.\nval model_xb = lr_xb.fit(training_ab)\nval predictions_xb = model_xb.transform(test_ab)\nval evaluator_xb = new RegressionEvaluator().setLabelCol(\"fscoreb\")\n\n\n\nprintln(\"Area under the ROC curve = \" + evaluator_xa.evaluate(predictions_xa)) \nprintln(\"Area under the ROC curve = \" + evaluator_xb.evaluate(predictions_xb)) \n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467139977061_-1243200350","id":"20160628-185257_1200125271","dateCreated":"Jun 28, 2016 6:52:57 PM","dateStarted":"Jul 14, 2016 11:39:59 AM","dateFinished":"Jul 14, 2016 11:40:26 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:914","errorMessage":""},{"title":"Print out model summaries and weights","text":"//println(model_summary_x.explainedVariance)\n//println(model_summary_x.featuresCol)\n//println(model_summary_x.objectiveHistory)\nval model_summary_xa = model_xa.summary\nprintln(model_summary_xa.r2)\nprintln(model_summary_xa.rootMeanSquaredError)\nprintln(model_xa.intercept)\nmodel_xa.weights\npredictions_xa.show(5)\n\nval model_summary_xb = model_xb.summary\nprintln(model_summary_xb.r2)\nprintln(model_summary_xb.rootMeanSquaredError)\nprintln(model_xb.intercept)\nmodel_xb.weights\npredictions_xb.show(5)\n//  .setInputCols(Array(\"scorea\", \"scoreb\", \"teama_adj_fscore\", \"teamb_adj_fscore\", \"timeleft\", \"overunder\", \"teama_vegas_fscore\", \"teamb_vegas_fscore\",\"teamaspread\" ))\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467140749643_1589796626","id":"20160628-190549_1590492207","dateCreated":"Jun 28, 2016 7:05:49 PM","dateStarted":"Jul 14, 2016 11:40:47 AM","dateFinished":"Jul 14, 2016 11:40:49 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:915","errorMessage":""},{"title":"Use an Evaluator to view model results","text":"import org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval evaluator = new RegressionEvaluator().setLabelCol(\"fscorea\")\n\nprintln(\"Area under the ROC curve = \" + evaluator.evaluate(predictions_a))","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467057060032_2074639352","id":"20160627-195100_207819365","dateCreated":"Jun 27, 2016 7:51:00 PM","dateStarted":"Jul 14, 2016 11:40:57 AM","dateFinished":"Jul 14, 2016 11:40:58 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:916","errorMessage":""},{"title":"RMS - double check ","text":"def calculateError( df:  org.apache.spark.sql.DataFrame, labelCol : String , predictionCol : String) : Double = {\n\n    val tmpdf = df.select(labelCol,predictionCol)\n                  .withColumn(\"rms\",((df(labelCol) - df(predictionCol)) * (df(labelCol) - df(predictionCol)) ))\n    val ad2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Double]\n    val rmssquare = ad2d(tmpdf.agg(avg(\"rms\")).first()(0))\n    scala.math.pow(rmssquare, 0.5)\n}\n\ncalculateError(predictions_a, \"fscorea\", \"prediction\")","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467074016038_-585074994","id":"20160628-003336_1247964680","dateCreated":"Jun 28, 2016 12:33:36 AM","dateStarted":"Jul 14, 2016 11:41:02 AM","dateFinished":"Jul 14, 2016 11:41:03 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:917","errorMessage":""},{"text":"//val trainingSummary = lrModel.summary\nprintln(s\"numIterations: ${model_summary_xb.totalIterations}\")\n//println(s\"objectiveHistory: ${model_summary_xb.objectiveHistory.toList}\")\n//model_summary_xb.residuals.show()\n//println(s\"RMSE: ${model_summary_xb.rootMeanSquaredError}\")\nprintln(s\"r2: ${model_summary_xb.r2}\"","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468376760288_4041312","id":"20160713-022600_28252872","dateCreated":"Jul 13, 2016 2:26:00 AM","dateStarted":"Jul 14, 2016 3:14:30 AM","dateFinished":"Jul 14, 2016 3:14:30 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:918","errorMessage":""},{"title":"Tune HyperParameters","text":"import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\nval paramGrid = new ParamGridBuilder()\n  .addGrid(lr_a.regParam, Array(0.01,0.03,0.1,0.3,1))\n  .build()\n\n\n//\n\nval cv_a = new CrossValidator()\n  .setEstimator(pipeline_a)\n  .setEvaluator(new RegressionEvaluator)\n  .setEstimatorParamMaps(paramGrid)\n  .setNumFolds(2) // Use 3+ in practice\n\n  // Run cross-validation, and choose the best set of parameters.\nval cvModel_a = cv_a.fit(trainingdf)","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467057010577_-629268649","id":"20160627-195010_1067263105","dateCreated":"Jun 27, 2016 7:50:10 PM","dateStarted":"Jul 14, 2016 3:14:30 AM","dateFinished":"Jul 14, 2016 3:14:30 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:919","errorMessage":""},{"title":"Evaluate CrossValidatation Model","text":"val cvpredictions_a = cvModel_a.transform(testdf)\nprintln(\"Area under the ROC curve for best fitted model = \" + evaluator.evaluate(cvpredictions_a))","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467079336522_-786454676","id":"20160628-020216_1241590914","dateCreated":"Jun 28, 2016 2:02:16 AM","dateStarted":"Jul 14, 2016 3:14:30 AM","dateFinished":"Jul 14, 2016 3:14:30 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:920","errorMessage":""},{"title":"Inspect the Model output","text":"cvpredictions_a.filter($\"timeleft\" > 32 && $\"timeleft\" < 36).show(300)","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"key","index":24,"aggr":"sum"}],"values":[{"name":"label","index":33,"aggr":"avg"},{"name":"prediction","index":35,"aggr":"avg"}],"groups":[],"scatter":{"xAxis":{"name":"timeleft","index":7,"aggr":"sum"},"yAxis":{"name":"label","index":33,"aggr":"sum"}},"lineWithFocus":false},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467079611264_608815570","id":"20160628-020651_1674766368","dateCreated":"Jun 28, 2016 2:06:51 AM","dateStarted":"Jul 14, 2016 3:14:30 AM","dateFinished":"Jul 14, 2016 3:14:30 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:921","errorMessage":""},{"title":"Model Parameters","text":"cvModel_a.explainParams()\nval lrcvModel_a.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(1)\ncvModel_a.parent.extractParamMap\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467136370821_-1507901015","id":"20160628-175250_1634913455","dateCreated":"Jun 28, 2016 5:52:50 PM","dateStarted":"Jul 14, 2016 3:14:30 AM","dateFinished":"Jul 14, 2016 3:14:30 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:922","errorMessage":""},{"title":"Linear Model Analysis And Explanation","text":"%md \n    \n    Complex Model Discussion\n                scoreb-scorea    timeleft     teamaspread      custom(score,time)\n    weights   =  0.077           0.006        -0.105           0.091  \n\n    score difference is exp(0.077) = 1.08 or 8 % increase in win likelihood with an increase in one more point !....\n    timeleft is not a strong predictor ...\n    teamspread is exp(-0.105) = 0.90 or 10 % change in likelihood of winning/losing based on spread change\n    \n    finally I added a feature that amplies the point differential as the game gets closer to finishing ...\n    it works something like this.  \n       if the home team is up by 10 at the beginning of the game, i scale that down to something like 5 points\n       if the home team is up by 10 at the end of the game, i scale that up to something like 20 points\n\n\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865199_2139664149","id":"20160627-021745_1758948833","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 3:13:57 AM","dateFinished":"Jul 14, 2016 3:13:58 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:923","errorMessage":""},{"title":"Function to predict new examples","text":"def getPrediction(teama : String  , scorea : Int , teamb : String , scoreb : Int,  timeleft : Double, teambspread : Double, model : org.apache.spark.ml.classification.LogisticRegressionModel) : Unit = {\n  val sd = scoreb-scorea\n  val sdt = scoredivtimeXform(sd.toDouble, timeleft.toDouble)\n  val lp = LabeledPoint(0.0,Vectors.dense(sd,timeleft,teambspread,sdt))\n\n  val single = Seq(lp).toDF(\"label\",\"features\")\n  val result = model.transform(single)\n  val rv = result.select($\"prediction\").head(1)\n  val prb = result.select($\"probability\").head(1)\n\n//scala.collection.immutable.IndexedSeq  \n  //.toDF(\"label\",\"features\")\n  val rv1 = ad2d(rv(0)(0))\n  val prb1 = prb(0)(0)\n  println(\"dbg : sdt =\" + sdt)\n  println(teama + \"(away) vs \" + teamb + \"(home)\")\n  println(\"Spread(HomeTeam) : \" + teambspread + \" (+ means home team is not favored)\")\n  println(\"Time Left        : \" + timeleft)\n  val winner = {if (rv1 == 1) teamb else teama}\n  println(\"Predicted Winner : \" + winner + \" Probablity : \" + prb1)\n}\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865199_2139664149","id":"20160627-021745_1963212696","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 3:14:30 AM","dateFinished":"Jul 14, 2016 3:14:31 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:924","errorMessage":""},{"title":"Simple Predictor based on a new Example ....","text":"// ADD in a game simulator here !!!\n\ngetPrediction(\"lac\",96, \"por\", 88, 2.0, -8.0, complexModel)\nprintln()\ngetPrediction(\"lac\",88, \"por\", 96, 2.0, -8.0, complexModel)\n\n","dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865199_2139664149","id":"20160627-021745_393008078","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 3:14:31 AM","dateFinished":"Jul 14, 2016 3:14:31 AM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:925","errorMessage":""},{"dateUpdated":"Jul 18, 2016 3:40:13 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993865199_2139664149","id":"20160627-021745_439515166","dateCreated":"Jun 27, 2016 2:17:45 AM","dateStarted":"Jul 14, 2016 3:14:31 AM","dateFinished":"Jul 14, 2016 3:14:31 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:926","errorMessage":""}],"name":"NBA-Predictions-2-LinearRegression","id":"2BQQ54C9T","angularObjects":{"2BQ66PMDN":[],"2BTK2EAKW":[],"2BSNG5M82":[],"2BQFDPZB2":[]},"config":{"looknfeel":"default"},"info":{}}