{"paragraphs":[{"title":"Imports","text":"import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport com.databricks.spark.csv\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\nimport scala.util.matching.Regex\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions.{udf,col,concat,lit,when}\n","dateUpdated":"Apr 27, 2016 8:54:09 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257360_207214219","id":"20160413-143737_2022518725","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 5:23:22 PM","dateFinished":"Apr 27, 2016 5:23:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69","errorMessage":""},{"title":"Cleanup old files ...","text":"%sh\n#rm -rf /data/resources/nbaSimpleModel\n#rm -rf /data/resources/nbaComplexModel","dateUpdated":"Apr 27, 2016 4:06:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461544465982_-1544452222","id":"20160425-003425_16053146","dateCreated":"Apr 25, 2016 12:34:25 AM","dateStarted":"Apr 27, 2016 4:06:44 PM","dateFinished":"Apr 27, 2016 4:06:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:70","errorMessage":""},{"title":"Helpers ... Lookup Tables","text":"println(\"Spark version = \" + sc.version)\nval teamMap = Map(\n  \"Atlanta\" -> \"atl\",\n  \"Boston\"  -> \"bos\",\n  \"Brooklyn\"  -> \"bkn\",\n  \"Charlotte\"  -> \"cha\",\n  \"Chicago\"  -> \"chi\",\n  \"Cleveland\"  -> \"cle\",\n  \"Dallas\"  -> \"dal\",\n  \"Denver\"  -> \"den\",\n  \"Detroit\"  -> \"det\",\n  \"Golden State\"  -> \"gst\",\n  \"Houston\"  -> \"hou\",\n  \"Indiana\"  -> \"ind\",\n  \"LA Clippers\"  -> \"lac\",\n  \"LA Lakers\"  -> \"lal\",\n  \"Memphis\"  -> \"mem\",\n  \"Miami\"  -> \"mia\",\n  \"Milwaukee\"  -> \"mil\",\n  \"Minnesota\"  -> \"min\",\n  \"New Orleans\"  -> \"nor\",\n  \"New York\"  -> \"nyk\",\n  \"Oklahoma City\"  -> \"okc\",\n  \"Orlando\"  -> \"orl\",\n  \"Philadelphia\"  -> \"phi\",\n  \"Phila.\"  -> \"phi\",\n  \"Phoenix\"  -> \"pho\",\n  \"Portland\"  -> \"por\",\n  \"Sacramento\" -> \"sac\",\n  \"San Antonio\"  -> \"san\",\n  \"Toronto\"  -> \"tor\",\n  \"Utah\"  -> \"uta\",\n  \"Washington\"  -> \"wsh\")\n  \nval monthMap = Map(\n    \"Jan\"-> \"01\",\n    \"Feb\"-> \"02\",\n    \"Mar\"-> \"03\",\n    \"Apr\"-> \"04\",\n    \"May\"-> \"05\",\n    \"Jun\"-> \"06\",\n    \"Jul\"-> \"07\",\n    \"Aug\"-> \"08\",\n    \"Sep\"-> \"09\",\n    \"Oct\"-> \"10\",\n    \"Nov\"-> \"11\",\n    \"Dec\"-> \"12\"\n)","dateUpdated":"Apr 27, 2016 5:23:28 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460563265570_-2093816832","id":"20160413-160105_660439088","dateCreated":"Apr 13, 2016 4:01:05 PM","dateStarted":"Apr 27, 2016 5:23:28 PM","dateFinished":"Apr 27, 2016 5:23:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71","errorMessage":""},{"title":"Inspect Score Data","text":"%sh\ncd /resources/data/\ngrep FINAL scores_nba.test.dat | head -n 3\necho\ngrep -v ET scores_nba.test.dat | head -n 3\n\n# Note we have both final scores and real time scores in this file.  This will be important later as we manipulate this data set\n ","dateUpdated":"Apr 27, 2016 5:23:42 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558795266_1439743379","id":"20160413-144635_156623904","dateCreated":"Apr 13, 2016 2:46:35 PM","dateStarted":"Apr 27, 2016 5:23:36 PM","dateFinished":"Apr 27, 2016 5:23:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72","errorMessage":""},{"title":"Interpret the Odds data","text":"%md\n\nHow to interpret the odds data ...\n    \n    Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n    Here Golden State is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n    being above/below that line.  \n    Finally the -125 / +145 numbers are whats known at the moneyline odds. \n        A negative number means you need to bet 125$ to get a 100$ payout\n        A positive number means you need to bet 100$ to get a 145$ payout","dateUpdated":"Apr 27, 2016 4:13:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461545195620_149810603","id":"20160425-004635_1932027239","dateCreated":"Apr 25, 2016 12:46:35 AM","dateStarted":"Apr 27, 2016 4:13:39 PM","dateFinished":"Apr 27, 2016 4:13:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:73","errorMessage":""},{"title":"Load in NBA Score Data Sets [R]","text":"\n// Since I dont have a header in the data set, I want to specify the column metadata\nval sqlContext = new SQLContext(sc)\nval customSchema = StructType(Array(\n    StructField(\"dateOrig\", DateType, true),\n    StructField(\"ts\", StringType, true),\n    StructField(\"teamlonga\", StringType, true),\n    StructField(\"scorea\", IntegerType, true),\n    StructField(\"teamlongb\", StringType, true),\n    StructField(\"scoreb\", IntegerType, true),\n    StructField(\"time-string\", StringType, true),\n    StructField(\"timeleft\", DoubleType, true),\n    StructField(\"gameid\", IntegerType, true)\n    ))\n\n// This line reads in the file and parses it with a CSV reader\nvar rtscoresAndFinalDF = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"false\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .option(\"mode\",\"DROPMALFORMED\")\n    .schema(customSchema)\n    .load(\"/resources/data/scores_nba.test.dat\") \n","dateUpdated":"Apr 27, 2016 5:27:22 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"tableHide":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257363_207598968","id":"20160413-143737_901570597","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 5:27:22 PM","dateFinished":"Apr 27, 2016 5:27:23 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74","errorMessage":""},{"title":"UDFs for creating extra columns in real time data frame","text":"// Create new team name column.. do simple lookup conversion with a UDF\nval mapper = teamin => teamMap(teamin)\nval mapperudf = udf(mapper)\n\n\n// Date Logic to adjust for games that finish on the day after ....\n// This is due to not having a great key to join my tables ...\nval datecrossregex = new Regex(\"^0[0-3]\")\nval dateadjust : ((String, String) => String) = (datein, tsin ) => {\n    val datetest =  datecrossregex.findFirstIn(tsin)\n    val dateary = datein.split(\"-\")\n    val rv = datetest match {\n      case Some(s) => { \n         val day = \"%02d\".format(dateary(2).toInt -1)\n         val newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day  \n         newdate\n      }\n      case None => datein\n    }\n    rv.asInstanceOf[String]\n}\nval dateadjustudf = udf(dateadjust)\n\n// UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n// Made this via intuition.  This can be extended to add other custom features\nval crossOverTime = 8\nval exponentScaler = 0.5\nval scoredivtimeXform: ((Double,Double) => Double) = (sd:Double, tl:Double) => (sd/(Math.pow( (tl / crossOverTime) + 0.1 , exponentScaler)))\nval scoredivtimeUdf = udf(scoredivtimeXform)\n\n","dateUpdated":"Apr 27, 2016 4:07:19 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461465592369_513037848","id":"20160424-023952_1534010312","dateCreated":"Apr 24, 2016 2:39:52 AM","dateStarted":"Apr 27, 2016 4:07:19 PM","dateFinished":"Apr 27, 2016 4:07:22 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:75","errorMessage":""},{"title":"Wrangle the Real Time and Final Score Data.  Add columns to the data set","text":"// Remove Overtime games from this analysis\nrtscoresAndFinalDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%OT%\"))\n\n// Create short 3 character team names \nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teama\", mapperudf(col(\"teamlonga\")))\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamb\", mapperudf(col(\"teamlongb\")))\n\n// Add a score differential Column \nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\n\n// Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n// Games adjusted to the day they started on.\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf($\"dateOrig\",$\"ts\"))\n\n// Create a Key for me to use to join with my odds data later.  Key = date.teama.teamb\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat($\"date\",lit(\".\"),$\"teama\",lit(\".\"),$\"teamb\"))","dateUpdated":"Apr 27, 2016 4:07:23 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461463388945_291064568","id":"20160424-020308_1122843137","dateCreated":"Apr 24, 2016 2:03:08 AM","dateStarted":"Apr 27, 2016 4:07:23 PM","dateFinished":"Apr 27, 2016 4:07:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:76","errorMessage":""},{"title":"Separate the Real Time and Final Data from One common Dataframe to two dataframes","text":"// Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file.  I need to pull these apart, and then join the dataframes so that I have a real time score and features, and know if the game was won or lost ....\n\n// Create Final Score DF\n// Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n\nvar finalscoresDF = rtscoresAndFinalDF.filter($\"time-string\".like(\"%FINAL%\"))\n\n// Rename some columns so that join later doesnt have name overlaps\nfinalscoresDF = finalscoresDF.withColumnRenamed(\"scorea\", \"fscorea\")\nfinalscoresDF = finalscoresDF.withColumnRenamed(\"scoreb\", \"fscoreb\")\n\n// Create final score difference\nfinalscoresDF = finalscoresDF.withColumn(\"fscorea-fscoreb\", $\"fscorea\" - $\"fscoreb\")\nfinalscoresDF = finalscoresDF.withColumn(\"fscoreb-fscorea\", $\"fscoreb\" - $\"fscorea\")\n\n// Add a Win/loss column Win = 1, Loss = 0\nfinalscoresDF = finalscoresDF.withColumn(\"away-win\", ($\"fscorea-fscoreb\" > 0).cast(\"double\"))\nfinalscoresDF = finalscoresDF.withColumn(\"home-win\", ($\"fscoreb-fscorea\" > 0).cast(\"double\"))\n\n// Remove Halftime records and these other cases as my datasource doesnt always change the quarter well\n// as this particular case isn't handled well... (for now)\nvar rtscoresDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%FINAL%\")).\n  filter(not($\"time-string\" === \"(HALFTIME)\")).\n  filter(not($\"time-string\" === \"(12:00 IN 1ST)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 2ND)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 3RD)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 4TH)\") ).\n  filter(not($\"time-string\" === \"(END OF 1ST)\") ).\n  filter(not($\"time-string\" === \"(END OF 2ND)\") ).\n  filter(not($\"time-string\" === \"(END OF 3RD)\") ).\n  filter(not($\"time-string\" === \"(END OF 4TH)\") )\n\n// Create real time score difference\nrtscoresDF = rtscoresDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\nrtscoresDF = rtscoresDF.withColumn(\"scoreb-scorea\", $\"scoreb\" - $\"scorea\")\n\n// Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\nrtscoresDF = rtscoresDF.withColumn(\"score-div-time\", scoredivtimeUdf($\"scoreb-scorea\", $\"timeleft\"))\n\n// Create a game PCT complete indictor\nrtscoresDF = rtscoresDF.withColumn(\"pct-complete\", ((($\"timeleft\" * -1) + 48 )/48.0)*100)\n","dateUpdated":"Apr 27, 2016 4:07:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461466019378_1651690444","id":"20160424-024659_1148847564","dateCreated":"Apr 24, 2016 2:46:59 AM","dateStarted":"Apr 27, 2016 4:07:34 PM","dateFinished":"Apr 27, 2016 4:07:35 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:77","errorMessage":""},{"title":"Inspect Odds Data","text":"%sh\ncd /resources/data/\nrm -rf  /resources/data/basketball_nba.xml\necho \"Daily Odds Info\"\nwget http://www.referincome.com/odds/rss2/basketball_nba.xml \ngrep title /resources/data/basketball_nba.xml\n\necho \necho \"Historical Odds Info\"\nhead -n 5 nbaodds_042516.xml\n\n\n","dateUpdated":"Apr 27, 2016 5:27:02 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558873002_1924465696","id":"20160413-144753_1684521837","dateCreated":"Apr 13, 2016 2:47:53 PM","dateStarted":"Apr 27, 2016 5:25:32 PM","dateFinished":"Apr 27, 2016 5:25:32 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:78","errorMessage":""},{"title":"Lets Take a Look of what we Have for the two dataframes we just wrangled","text":"\n// Some Printouts .....\nprintln(\"final scores data frame\")\nfinalscoresDF.show(5)\nprintln(\"real time scores data frame\")\nrtscoresDF.show(5)\n\n//rtscoresDF.filter($\"key\" === \"2016-04-10.orl.mia\").show(350)\n\n","dateUpdated":"Apr 27, 2016 4:07:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461466795437_1840332897","id":"20160424-025955_1123703578","dateCreated":"Apr 24, 2016 2:59:55 AM","dateStarted":"Apr 27, 2016 4:07:37 PM","dateFinished":"Apr 27, 2016 4:07:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:79","errorMessage":""},{"title":"Load in Odds data","text":"// Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n// This is not beautiful, but gets the job done\n// Data format .....\n//       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n//       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n\n// Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\nval oddsrdd = sc.textFile(\"/resources/data/nbaodds_042516.xml\")\n\n// just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\nval gameStringRdd = oddsrdd.map(x => x.substring(x.indexOf('>')+1,x.lastIndexOf('<')))\n\n// String to Double converter helper\ndef parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ : Throwable => None }\n\n// This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n// just lots of string parsing and data type conversions\nval oddsDF = gameStringRdd.map(x => {\n  // find the period, and then find the space prior ot the period\n  // Philadelphia special case.  since later on I index on a decimal point, I am assuming its part of a number and not a team abbrev\n  // also removing commas with the cryptic filterNot section ... essentially \n   val x1 = x.replace(\"Phila.\", \"Philadelphia\").filterNot(\",\" contains _)\n   val ss1 = x1.substring(0,x1.indexOf('.'))\n   val teamlonga = ss1.substring(0,ss1.lastIndexOf(' '))\n   val teama = teamMap(teamlonga)\n   val ss2 = x1.substring(ss1.lastIndexOf(' ')+1,x1.length)\n   val teamaspread = parseDouble(ss2.substring(0,ss2.indexOf(' ')) )\n   val ss3 = ss2.substring(ss2.indexOf(' ')+1,ss2.length)\n   val overunder = parseDouble(ss3.substring(ss3.indexOf('(')+1,ss3.indexOf(')')))\n   val ss4 = ss3.substring(ss3.indexOf(')')+2,ss3.length)\n   val teamaml = parseDouble(ss4.substring(0,ss4.indexOf(' ')))\n   val ss5x = ss4.substring(ss4.indexOf('|')+2,ss4.length)\n   val ss5 = ss5x.substring(0,ss5x.indexOf('.'))\n   \n   val teamlongb = ss5.substring(0,ss5.lastIndexOf(' '))\n   val teamb = teamMap(teamlongb)\n   val ss6 = ss5x.substring(ss5.lastIndexOf(' ')+1,ss5x.length)\n\n   val teambml = parseDouble(ss6.substring(ss6.indexOf(')')+2,ss6.lastIndexOf('(')-1))\n   val ss7 = ss6.substring(ss6.lastIndexOf('(')+1,ss6.length)\n   val dateInfo = ss7.split(' ')\n   val dateStr = dateInfo(2) + \"-\" + monthMap(dateInfo(0)) + \"-\" + dateInfo(1)\n   // This will become my join key for the other data sets\n   val key = dateStr +\".\" + teama + \".\" + teamb\n  (key,teamlonga,teama,teamaspread,overunder,teamaml,teamlongb,teamb,teambml,dateStr)\n   \n  }\n  // Had to add this below because I was getting the game odds over multiple days, and it was \n  // adding noise to the analysis\n).toDF(\"key\",\"teamlonga\",\"teama\",\"teamaspread\",\"overunder\",\"teamaml\",\"teamlongb\",\"teamb\",\"teambml\",\"dateStr\")\n    .groupBy(\"key\",\"teamlonga\",\"teama\",\"teamlongb\",\"teamb\",\"dateStr\")\n    .agg(avg(\"teamaspread\"),avg(\"overunder\"),avg(\"teamaml\"),avg(\"teambml\"))\n    .withColumnRenamed(\"avg(teamaspread)\",\"teamaspread\")\n    .withColumnRenamed(\"avg(overunder)\",\"overunder\")\n    .withColumnRenamed(\"avg(teamaml)\",\"teamaml\")\n    .withColumnRenamed(\"avg(teambml)\",\"teambml\")\n    .withColumn(\"teambspread\", $\"teamaspread\" * -1)\n\noddsDF.registerTempTable(\"oddsDF\")\n\n\n","dateUpdated":"Apr 27, 2016 4:07:42 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460561107693_-718050523","id":"20160413-152507_1919365936","dateCreated":"Apr 13, 2016 3:25:07 PM","dateStarted":"Apr 27, 2016 4:07:42 PM","dateFinished":"Apr 27, 2016 4:07:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:80","errorMessage":""},{"title":"Inspect some of the Odds Data","text":"oddsDF.show(5)\nprintf(\"Total Home Teams      = %2d\\n\",oddsDF.select(\"teama\").distinct.sort(\"teama\").count())\nprintf(\"Total Away Teams      = %2d\\n\",oddsDF.select(\"teamb\").distinct.sort(\"teamb\").count())\nprintf(\"Total Games Collected = %d\\n \",oddsDF.count())\n\n//oddsDF.filter($\"key\" === \"2016-04-08.mil.bos\")\n//    .groupBy(\"key\",\"teamlonga\",\"teama\",\"teamlongb\",\"teamb\",\"dateStr\")\n//    .agg(avg(\"teamaspread\"),avg(\"overunder\"),avg(\"teamaml\"),avg(\"teambml\"))\n//    .withColumnRenamed(\"avg(teamaspread)\",\"teamaspread\")\n//    .withColumnRenamed(\"avg(overunder)\",\"overunder\")\n//    .withColumnRenamed(\"avg(teamaml)\",\"teamaml\")\n//    .withColumnRenamed(\"avg(teambml)\",\"teambml\")\n//    .show()\n    \n","dateUpdated":"Apr 27, 2016 4:07:44 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"date","index":0,"aggr":"sum"}],"values":[{"name":"ts","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"date","index":0,"aggr":"sum"},"yAxis":{"name":"ts","index":1,"aggr":"sum"}}},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257363_207598968","id":"20160413-143737_1963307275","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 4:07:45 PM","dateFinished":"Apr 27, 2016 4:08:02 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:81","errorMessage":""},{"title":"Join The Odds and Final Score data sets","text":"// Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n\n// Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\nval finalslicedscoresDF = finalscoresDF.select($\"key\",$\"fscorea\",$\"fscoreb\",$\"fscorea-fscoreb\",$\"fscoreb-fscorea\",$\"away-win\",$\"home-win\")\n\n// First Join the 2 smallest data frames ... odd and final.\nval gameDF = oddsDF.join(finalslicedscoresDF, oddsDF(\"key\") === finalscoresDF(\"key\")).drop(oddsDF(\"key\"))\n.drop(\"teamlonga\")\n.drop(\"teamlongb\")\n.drop(\"teama\")\n.drop(\"teamb\")\n\n// Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\nprintln(\"gameDF\")\ngameDF.show(3)\nprintf(\"Total Joined Games Collected = %d\\n \",gameDF.count())\ngameDF.registerTempTable(\"gameDF\")\n","dateUpdated":"Apr 27, 2016 4:07:47 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true,"title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460599590051_-289125386","id":"20160414-020630_55119635","dateCreated":"Apr 14, 2016 2:06:30 AM","dateStarted":"Apr 27, 2016 4:07:47 PM","dateFinished":"Apr 27, 2016 4:08:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:82","errorMessage":""},{"title":"Lets see if there are some correlations ... Spread vs Final Score Difference","text":"//z.show(gameDF.filter($\"key\".contains(\"gst\")))\nz.show(gameDF)\n// here we show that the better a team is (negative spread, the more they are likely to win ...)\n","dateUpdated":"Apr 27, 2016 4:09:14 PM","config":{"colWidth":6,"graph":{"mode":"scatterChart","height":326,"optionOpen":true,"keys":[{"name":"dateStr","index":0,"aggr":"sum"}],"values":[{"name":"teamaspread","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"fscoreb-fscorea","index":10,"aggr":"sum"},"yAxis":{"name":"teambspread","index":5,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461637361668_-1851920027","id":"20160426-022241_281434645","dateCreated":"Apr 26, 2016 2:22:41 AM","dateStarted":"Apr 27, 2016 4:08:15 PM","dateFinished":"Apr 27, 2016 4:08:19 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:83","errorMessage":""},{"title":"Home / Away sensitivity to Point Spread ","text":"%sql\n\nselect teambspread, COUNT(fscorea) as CNT_AWAY, AVG(fscorea) as AVG_AWAY, COUNT(fscoreb) as CNT_HOME, AVG(fscoreb) as AVG_HOME from gameDF group by teambspread order by teambspread\n\n-- Note the light blue line (Home) is above the dark blue (Away) when they are expected to win.. and even when the spread is negative for the away team, the hometeam still has the better average score in some cases ...\n","dateUpdated":"Apr 27, 2016 4:10:25 PM","config":{"colWidth":6,"graph":{"mode":"scatterChart","height":384,"optionOpen":true,"keys":[{"name":"teambspread","index":0,"aggr":"sum"}],"values":[{"name":"AVG_AWAY","index":2,"aggr":"avg"},{"name":"AVG_HOME","index":4,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"teambspread","index":0,"aggr":"sum"},"yAxis":{"name":"AVG_HOME","index":4,"aggr":"sum"}},"lineWithFocus":false},"enabled":true,"editorMode":"ace/mode/sql","title":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461637361006_1320349979","id":"20160426-022241_1818330184","dateCreated":"Apr 26, 2016 2:22:41 AM","dateStarted":"Apr 27, 2016 4:09:06 PM","dateFinished":"Apr 27, 2016 4:09:13 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:84","errorMessage":""},{"title":"Join The Game Dataframe with the real time Score dataframe","text":"// This is the bigger merge.  Merging the odds/final score data with the real time indicators ...\n\nval lrDF = rtscoresDF.join(gameDF, rtscoresDF(\"key\") === gameDF(\"key\")).drop(gameDF(\"key\"))\nprintln(\"lrDF : Logistic Regression Data Frame\")\nlrDF.show(3)\nprintf(\"Total Data Points in DataSet = %d\\n \",lrDF.count())\nlrDF.registerTempTable(\"lrDF\")\n\n\n","dateUpdated":"Apr 27, 2016 4:10:30 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461467755742_515777586","id":"20160424-031555_39801048","dateCreated":"Apr 24, 2016 3:15:55 AM","dateStarted":"Apr 27, 2016 4:10:30 PM","dateFinished":"Apr 27, 2016 4:10:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85","errorMessage":""},{"title":"Lets Look at some stats from logistic Regression dataframe","text":"\n lrDF.describe().show()","dateUpdated":"Apr 27, 2016 4:14:11 PM","config":{"colWidth":12,"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[],"values":[],"groups":[],"scatter":{"xAxis":{"name":"dateOrig","index":0,"aggr":"sum"},"yAxis":{"name":"ts","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460686526877_-9790501","id":"20160415-021526_1114313017","dateCreated":"Apr 15, 2016 2:15:26 AM","dateStarted":"Apr 27, 2016 4:14:11 PM","dateFinished":"Apr 27, 2016 4:14:18 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:86","errorMessage":""},{"title":"Debug only - Reduce data","text":"// Shrink the data just for now ...\nval splits = lrDF.randomSplit(Array(0.75,0.24,0.01), seed = 11L)\nval lrDF = splits(1).cache()\n\n","dateUpdated":"Apr 27, 2016 4:05:38 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461637230741_-1408259096","id":"20160426-022030_765857853","dateCreated":"Apr 26, 2016 2:20:30 AM","dateStarted":"Apr 26, 2016 1:40:57 PM","dateFinished":"Apr 26, 2016 1:40:57 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"title":"Visualize some of our Data. .... San Antonio is pretty good","text":"//z.show(lrDF.filter($\"key\".endsWith(\"gst\")))\nz.show(lrDF.filter($\"key\".endsWith(\"san\") || $\"key\".endsWith(\"gst\")))\n//z.show(lrDF.filter($\"key\".contains(\".san.\")))\n// here we can see the trajectory of some of the games .....    \n// upper left beginning ... upper right (win), lower right (loss)","dateUpdated":"Apr 27, 2016 5:36:50 PM","config":{"colWidth":6,"graph":{"mode":"scatterChart","height":300,"optionOpen":true,"keys":[{"name":"dateOrig","index":0,"aggr":"sum"}],"values":[{"name":"ts","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"pct-complete","index":15,"aggr":"sum"},"yAxis":{"name":"scoreb-scorea","index":13,"aggr":"sum"},"group":{"name":"key","index":22,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461468118719_-1234138677","id":"20160424-032158_947636172","dateCreated":"Apr 24, 2016 3:21:58 AM","dateStarted":"Apr 27, 2016 5:36:47 PM","dateFinished":"Apr 27, 2016 5:36:59 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:88","errorMessage":""},{"title":"Brooklyn and Philly aren't as good....","text":"z.show(lrDF.filter($\"key\".endsWith(\".bkn\") || $\"key\".endsWith(\".phi\" )))\n\n\n","dateUpdated":"Apr 27, 2016 5:37:01 PM","config":{"colWidth":6,"graph":{"mode":"scatterChart","height":300,"optionOpen":true,"keys":[{"name":"dateOrig","index":0,"aggr":"sum"}],"values":[{"name":"ts","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"pct-complete","index":15,"aggr":"sum"},"yAxis":{"name":"scoreb-scorea","index":13,"aggr":"sum"},"group":{"name":"key","index":22,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461623299480_1328664943","id":"20160425-222819_450926645","dateCreated":"Apr 25, 2016 10:28:19 PM","dateStarted":"Apr 27, 2016 5:36:58 PM","dateFinished":"Apr 27, 2016 5:37:08 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:89","errorMessage":""},{"title":"Samples per Game Visualization - Data Quality check [opt]","text":"// This shows I need to work on my data wrangling a bit more ....\n//z.show(lrDF.groupBy(\"key\").agg(avg(\"fscoreb\"),count(\"key\")).orderBy(asc(\"count(key)\")) )\n\nz.show(lrDF.groupBy(\"key\").agg(count(\"key\")).orderBy(asc(\"count(key)\")) )\n//z.show(lrDF.filter($\"key\" === \"2016-04-22.atl.bos\") )\n\n","dateUpdated":"Apr 27, 2016 5:38:35 PM","config":{"colWidth":12,"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[{"name":"key","index":0,"aggr":"sum"}],"values":[{"name":"count(key)","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"key","index":0,"aggr":"sum"},"yAxis":{"name":"count(key)","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461642019556_-760674220","id":"20160426-034019_1460950453","dateCreated":"Apr 26, 2016 3:40:19 AM","dateStarted":"Apr 27, 2016 5:38:32 PM","dateFinished":"Apr 27, 2016 5:38:37 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90","errorMessage":""},{"title":"'pack' Logistic dataframe into required format for Logistic regression [R] - Creating a simple/complex dataframe for comparison","text":"// Logistic regression requires that\n// The best way I found to modify data in a custom way is with map, however it returns an RDD, so you will need to run toDF at the end\n// Logistic DF requires a DF of type  => [label: double, features: vector]\n\nimport org.apache.spark.sql._\n// These is a helper function that converts an 'Any(Int)' type to a Double or Any(Double) to a Double\nval ai2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Integer].doubleValue\nval ad2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Double]\n\nvar nbaSimpleLrDF = lrDF.map {\n    dfrow => {\n    // get the indexes ... prob should move this out of the looop!\n      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"home-win\")))\n      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scoreb-scorea\")))\n      val f2 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n      LabeledPoint(tgt,Vectors.dense(f1,f2))\n    }\n}.toDF(\"label\",\"features\")\n\nvar nbaComplexLrDF = lrDF.map {\n    dfrow => {\n    // get the indexes ... prob should move this out of the looop!\n      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"home-win\")))\n      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scoreb-scorea\")))\n      val f2 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n      val f3  = ad2d( dfrow(dfrow.fieldIndex(\"teambspread\")))\n      //val f3  = ad2d( dfrow(dfrow.fieldIndex(\"overunder\")))\n      //val f4  = ad2d( dfrow(dfrow.fieldIndex(\"teamaml\")))\n      // Add a new feature that weight point differential more as time left gets smaller...\n      // I played around with excel and this looked ok ....\n      val f4  = ad2d( dfrow(dfrow.fieldIndex(\"score-div-time\")))\n      LabeledPoint(tgt,Vectors.dense(f1,f2,f3,f4))\n    }\n}.toDF(\"label\",\"features\")\n\n\nnbaSimpleLrDF.show(3)\nnbaComplexLrDF.show(3)\n//nbalrDF = nbalrDF.withColumn(\"new\", $\"label\")\n\n","dateUpdated":"Apr 27, 2016 4:14:23 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"time-left","index":0,"aggr":"sum"}],"values":[{"name":"team-a-differential","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"time-left","index":0,"aggr":"sum"},"yAxis":{"name":"team-a-differential","index":1,"aggr":"sum"}}},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257363_207598968","id":"20160413-143737_531560843","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 4:14:23 PM","dateFinished":"Apr 27, 2016 4:14:30 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:91","errorMessage":""},{"title":"Function to Create the Model and Train it and Test it ","text":"def trainAndTest( indf : org.apache.spark.sql.DataFrame, modelPath : String ) : (org.apache.spark.sql.DataFrame, org.apache.spark.ml.classification.LogisticRegressionModel)  = {\n    val splits = indf.randomSplit(Array(0.60,0.39,0.01), seed = 11L)\n    val trainingdf = splits(0).cache()\n    val testdf = splits(1).cache()\n    val crossvaldf = splits(2).cache()\n    \n    println(\"Tranining Samples = \" + trainingdf.count())\n    println(\"Test      Samples = \" + testdf.count())\n    println(\"Cross Val Samples = \" + crossvaldf.count())\n    \n    // Setup some of the configurations for the Logistic regression model ..\n    // Here we could try a pipeline with params to select the 'best setting' but in the interest of time will go with this\n    val lr = new LogisticRegression()\n      .setMaxIter(75)\n    .setRegParam(0.01)\n    .setElasticNetParam(0.0)\n\n    // Fit the model\n    val lrModel   = lr.fit(trainingdf)\n\n    println(\"Reg Parameter:    =\" + lrModel.getRegParam)\n    println(\"lrModel.intercept = \" + lrModel.intercept)\n    println(\"lrModel.weights   = \" + lrModel.weights)\n\n    // Save the model for later use ....\n    // Argh ! -> in 1.6.1 api, but not 1.5.2 :(  \n    // lrModel.save(\"modelPath\" )\n    \n    ////  Create a logistic regression summary object ////\n    // val lrSummary = lrModel.summary\n    // println(\"lrSummary.objectiveHistory = \" + lrSummary.objectiveHistory.length)\n    // println(lrSummary.objectiveHistory.deep.mkString(\"\\n\"))\n    ////\n    \n    // transform is now used in lieu of predict from mllib.  Found this after studying the API for a while\n    val predictions = lrModel.transform(testdf)\n\n    // Select (prediction, true label) and compute test error\n    val evaluator = new MulticlassClassificationEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"precision\")\n    \n    val accuracy = evaluator.evaluate(predictions)\n    println(\"Test Error = \" + (1.0 - accuracy))\n\n    // return the \n    (predictions,lrModel)\n}\n","dateUpdated":"Apr 27, 2016 5:40:29 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"features","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"features","index":1,"aggr":"sum"}}},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257363_207598968","id":"20160413-143737_1101153164","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 5:40:29 PM","dateFinished":"Apr 27, 2016 5:40:29 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:92","errorMessage":""},{"title":"Test and Train multiple models","text":"val (simplePredictionDF, simpleModel)   = trainAndTest(nbaSimpleLrDF, \"/data/resources/nbaSimpleModel\")\nprintln()\nval (complexPredictionDF, complexModel) = trainAndTest(nbaComplexLrDF, \"/data/resources/nbaComplexModel\")\n\n","dateUpdated":"Apr 27, 2016 5:41:30 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"prediction","index":4,"aggr":"count"}],"groups":[{"name":"prediction","index":4,"aggr":"sum"}],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"features","index":1,"aggr":"sum"}}},"enabled":true,"title":true,"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460855788550_2107812970","id":"20160417-011628_2142676999","dateCreated":"Apr 17, 2016 1:16:28 AM","dateStarted":"Apr 27, 2016 5:40:36 PM","dateFinished":"Apr 27, 2016 5:40:59 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:93","errorMessage":""},{"title":"Simple vs Complex Interpretation ","text":"%md\nwhere to go from here?\n\n    The complex model, with just 2 extra features added provides an additional 5% improvement in predictions ...\n    Simple accuracy = 78.6%\n    Complex accuracy = 83.7%\n    Further analysis could be a per quarter breakdown ......\n    \n    To figure out where to go next, machine learning techniques involving tuning model paramters, possibly acquiring more data, and adding more features can be pursued.\n    For our example, we will move on\n","dateUpdated":"Apr 27, 2016 5:41:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461772391321_-2130181221","id":"20160427-155311_2013244670","dateCreated":"Apr 27, 2016 3:53:11 PM","dateStarted":"Apr 27, 2016 5:41:36 PM","dateFinished":"Apr 27, 2016 5:41:37 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:94","errorMessage":""},{"title":"Inspect the model output","text":"complexPredictionDF.show(50)","dateUpdated":"Apr 27, 2016 4:15:23 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461771083124_-413488516","id":"20160427-153123_1225304841","dateCreated":"Apr 27, 2016 3:31:23 PM","dateStarted":"Apr 27, 2016 4:15:13 PM","dateFinished":"Apr 27, 2016 4:15:14 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:95","errorMessage":""},{"title":"Logistic Analysis And Explanation","text":"%md \n    \n    Complex Model Discussion\n                scoreb-scorea    timeleft     teamaspread      custom(score,time)\n    weights   =  0.077           0.006        -0.105           0.091  \n\n    score difference is exp(0.077) = 1.08 or 8 % increase in win likelihood with an increase in one more point !....\n    timeleft is not a strong predictor ...\n    teamspread is exp(-0.105) = 0.90 or 10 % change in likelihood of winning/losing based on spread change\n    \n    finally I added a feature that amplies the point differential as the game gets closer to finishing ...\n    it works something like this.  \n       if the home team is up by 10 at the beginning of the game, i scale that down to something like 5 points\n       if the home team is up by 10 at the end of the game, i scale that up to something like 20 points\n\n\n","dateUpdated":"Apr 27, 2016 4:15:25 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","tableHide":true,"editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460856662182_2117116462","id":"20160417-013102_393252246","dateCreated":"Apr 17, 2016 1:31:02 AM","dateStarted":"Apr 27, 2016 4:15:26 PM","dateFinished":"Apr 27, 2016 4:15:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:96","errorMessage":""},{"title":"Function To Convert Features Vector into a standard dataframe","text":"// The whole point to doing this is so that I can visualize the features and the outcomes.  Zeppelin cannot render the Vector type well at all, so converting the data back to a flattened style of layout\n// Expect a column name to be passed that is the features vector, all other columns should come back as-is\n// I could extend the MLLIB DF to do this !! and  do it\n\n// Create a Row from values.\n// Row(value1, value2, value3, ...)\n// Create a Row from a Seq of values.\n// Row.fromSeq(Seq(value1, value2, ...))\n\n// Goal of this function to return a new flattened data frame with the Vector datatype replaced with multiple columns.  This\n// is done so that I can use zeppelin to graph my data since it doesnt really handle vectors well for graphing.\n// not super flexible as of now, but someday might make it better ...\n\ndef convertVectorDF (indf : org.apache.spark.sql.DataFrame, labelIndex : Int, featureIndex : Int, probIndex : Int, predictionIndex : Int) = {\n    //indf.select(colname).show(5)\n    val myCols = indf.columns\n    \n    // debug stuff\n    //println(\"featureIndex value = \" + myCols(featureIndex))\n    //println(\"probIndex value = \" + myCols(probIndex))\n    //println(\"predictionIndex value = \" + myCols(predictionIndex))\n    \n    if(myCols(labelIndex) != \"label\") {\n        println(\"labelIndex value = \" + myCols(labelIndex))\n        println(\"Error\")\n    }\n    //println(\" myCols.length = \" + myCols.length)\n    //println(\"Vector Size = \" + indf[0].features)\n    //indf.show(3)\n    \n    // build an RDD of sql Rows here, then use toDF to convert back to a Dataframe..\n    val tmprdd = indf.map { line => \n        var reind = 0\n        var newRow : Seq[Double]= Seq(line(labelIndex).asInstanceOf[Double])\n        var labeledPointVector = line(featureIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n        for(i <- 0 to labeledPointVector.size -1 ) {\n            newRow = newRow :+ labeledPointVector(i).asInstanceOf[Double]\n        }\n        var probArray = line(probIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n        newRow = newRow :+ probArray(0).asInstanceOf[Double]\n        newRow = newRow :+ line(predictionIndex).asInstanceOf[Double]\n        //  https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/Row.html see example where \n        //  Row.fromSeq is used to make a Row\n        Row.fromSeq(newRow)\n    }\n    //http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n    //https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/types/StructType.html\n\n    // Take the first row. Each row just has a single 'Row' object at position 0\n    val numCols = tmprdd.take(1)(0).length\n    var schemaString = myCols(labelIndex);\n    for(i <-0 to numCols - 4){\n        schemaString = schemaString + \",f\" +i \n    }\n    schemaString = schemaString + \",\" + myCols(probIndex);\n    schemaString = schemaString + \",\" + myCols(predictionIndex);\n    //println(schemaString)\n    val schema  = StructType(schemaString.split(\",\").map(fieldName => StructField(fieldName,DoubleType,true)))\n    val finalDF = sqlContext.createDataFrame(tmprdd, schema)\n    finalDF\n}","dateUpdated":"Apr 27, 2016 4:15:29 PM","config":{"lineNumbers":false,"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"f0","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"f0","index":1,"aggr":"sum"}}},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257364_205675223","id":"20160413-143737_1723046446","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 4:15:29 PM","dateFinished":"Apr 27, 2016 4:15:30 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:97","errorMessage":""},{"title":"Build an Feature Evaluation Grid For Complex Model","text":"\n// This creates my evaluation grid..  It creates a vector(vector(vector ....)\nval nestedVectors0 = Range(-20, 20, 1).map( sd => { \n    val nestedVectors1 = Range(0,20,1).map( tl => { \n        val nestedVectors2 = Range(-10,-8,2).map( spr => {\n            val overunder = 200\n            val mla = spr * 46.2 - 8.2 // roughly calculated by eyeballing the oddsDF.  I dont think mla has any extra info....\n            val sdt = scoredivtimeXform(sd.toDouble, tl.toDouble)\n            //val r = scala.util.Random\n            //val scaler = r.nextFloat/100 +1.0\n            val rv = LabeledPoint(0.0,Vectors.dense(sd,tl,spr,sdt))\n            rv\n        })\n        nestedVectors2\n    })\n    nestedVectors1\n})\n\n// De-nest this data structure ...\nval evalcomplexDF = nestedVectors0.flatMap( x => {\n    x.flatMap( y => y)}).toDF(\"label\",\"features\")\n\nval complexGridPredictionsDF = complexModel.transform(evalcomplexDF)\n//complexGridPredictionsDF.show(4)\n","dateUpdated":"Apr 27, 2016 4:16:09 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461523377890_-1028506720","id":"20160424-184257_1064181190","dateCreated":"Apr 24, 2016 6:42:57 PM","dateStarted":"Apr 27, 2016 4:15:53 PM","dateFinished":"Apr 27, 2016 4:15:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:98","errorMessage":""},{"title":"File Cleanup","text":"%sh\nrm -rf /resources/data/complexGridPredictionsUnpackedDF.csv\nrm -rf /resources/data/simplePredictionsUnpackedDF.csv\nrm -rf /resources/data/complexPredictionsUnpackedDF.csv","dateUpdated":"Apr 27, 2016 4:15:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","editorHide":true,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461614358876_-727011130","id":"20160425-195918_1343724748","dateCreated":"Apr 25, 2016 7:59:18 PM","dateStarted":"Apr 27, 2016 4:15:34 PM","dateFinished":"Apr 27, 2016 4:15:34 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:99","errorMessage":""},{"title":"Lets Publish The data and Model for potential  visualizations in other tools","text":"\n// index 0 = label\n// index 1 = features\n// index 3 = probability\n// index 4 = prediction\nval complexGridPredictionsUnpackedDF = convertVectorDF(complexGridPredictionsDF,0,1,3,4).withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0)).\nwithColumnRenamed(\"f0\", \"scoreb-scorea\").\nwithColumnRenamed(\"f1\", \"timeleft\").\nwithColumnRenamed(\"f2\", \"teambspread\").\nwithColumnRenamed(\"f3\", \"score-div-time\")\n\nval simplePredictionsUnpackedDF = convertVectorDF(simplePredictionDF,0,1,3,4).withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0)).\nwithColumnRenamed(\"f0\", \"scoreb-scorea\").\nwithColumnRenamed(\"f1\", \"timeleft\")\n\nval complexPredictionsUnpackedDF = convertVectorDF(complexPredictionDF,0,1,3,4).withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0)).\nwithColumnRenamed(\"f0\", \"scoreb-scorea\").\nwithColumnRenamed(\"f1\", \"timeleft\").\nwithColumnRenamed(\"f2\", \"teambspread\").\nwithColumnRenamed(\"f3\", \"score-div-time\")\n\ncomplexGridPredictionsUnpackedDF.write.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"false\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .option(\"mode\",\"DROPMALFORMED\")\n    .save(\"/resources/data/complexGridPredictionsUnpackedDF.csv\")\n\nsimplePredictionsUnpackedDF.write.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"false\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .option(\"mode\",\"DROPMALFORMED\")\n    .save(\"/resources/data/simplePredictionsUnpackedDF.csv\")\n    \ncomplexPredictionsUnpackedDF.write.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"false\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .option(\"mode\",\"DROPMALFORMED\")\n    .save(\"/resources/data/complexPredictionsUnpackedDF.csv\")","dateUpdated":"Apr 27, 2016 4:16:28 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"f0","index":1,"aggr":"sum"}],"groups":[{"name":"label","index":0,"aggr":"sum"}],"scatter":{"xAxis":{"name":"f1","index":2,"aggr":"sum"},"yAxis":{"name":"f0","index":1,"aggr":"sum"},"group":{"name":"correct","index":5,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461375610219_-965007321","id":"20160423-014010_1930561022","dateCreated":"Apr 23, 2016 1:40:10 AM","dateStarted":"Apr 27, 2016 4:15:58 PM","dateFinished":"Apr 27, 2016 4:16:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:100","errorMessage":""},{"title":"Function to predict new examples","text":"def getPrediction(teama : String  , scorea : Int , teamb : String , scoreb : Int,  timeleft : Double, teambspread : Double, model : org.apache.spark.ml.classification.LogisticRegressionModel) : Unit = {\n  val sd = scoreb-scorea\n  val sdt = scoredivtimeXform(sd.toDouble, timeleft.toDouble)\n  val lp = LabeledPoint(0.0,Vectors.dense(sd,timeleft,teambspread,sdt))\n\n  val single = Seq(lp).toDF(\"label\",\"features\")\n  val result = model.transform(single)\n  val rv = result.select($\"prediction\").head(1)\n  val prb = result.select($\"probability\").head(1)\n\n//scala.collection.immutable.IndexedSeq  \n  //.toDF(\"label\",\"features\")\n  val rv1 = ad2d(rv(0)(0))\n  val prb1 = prb(0)(0)\n  println(\"dbg : sdt =\" + sdt)\n  println(teama + \"(away) vs \" + teamb + \"(home)\")\n  println(\"Spread(HomeTeam) : \" + teambspread + \" (+ means home team is not favored)\")\n  println(\"Time Left        : \" + timeleft)\n  val winner = {if (rv1 == 1) teamb else teama}\n  println(\"Predicted Winner : \" + winner + \" Probablity : \" + prb1)\n}\n","dateUpdated":"Apr 27, 2016 5:44:27 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461772151914_-2095106130","id":"20160427-154911_588738830","dateCreated":"Apr 27, 2016 3:49:11 PM","dateStarted":"Apr 27, 2016 5:44:27 PM","dateFinished":"Apr 27, 2016 5:44:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:101","errorMessage":""},{"title":"Simple Predictor based on a new Example ....","text":"// ADD in a game simulator here !!!\n\ngetPrediction(\"lac\",96, \"por\", 88, 2.0, -8.0, complexModel)\nprintln()\ngetPrediction(\"lac\",88, \"por\", 96, 2.0, -8.0, complexModel)\n\n","dateUpdated":"Apr 27, 2016 4:16:38 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461644307030_-2089364891","id":"20160426-041827_72730405","dateCreated":"Apr 26, 2016 4:18:27 AM","dateStarted":"Apr 27, 2016 3:52:34 PM","dateFinished":"Apr 27, 2016 3:52:34 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"title":"Next Steps and ideas","text":"%md\ntopic list\n\n    Add single team dataframe ...\n    Add team features .... that will make the graph very large\n    Parameterize the model and select the best threshold // regularizaiton\n    study learning curves\n    switch to linear regression as I am losing some information with just W/L\n    use a case class for model ....\n    add an outlier checker for bad data in games .... \n    add a udf to convert END OF cases // 12:00 in cases to proper time .. but maybe not\n    or just fix my script that captures the data\n\n","dateUpdated":"Apr 27, 2016 6:19:26 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460558257365_205290474","id":"20160413-143737_200673168","dateCreated":"Apr 13, 2016 2:37:37 PM","dateStarted":"Apr 27, 2016 6:19:26 PM","dateFinished":"Apr 27, 2016 6:19:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:103","errorMessage":""},{"dateUpdated":"Apr 27, 2016 4:05:38 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461764647195_186110459","id":"20160427-134407_1262281518","dateCreated":"Apr 27, 2016 1:44:07 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"}],"name":"NBA-Predictions","id":"2BJQZ37V7","angularObjects":{"2BFUVZUYX":[],"2BJBA27BJ":[],"2BFSWFDUA":[],"2BGUZEEFU":[]},"config":{"looknfeel":"default"},"info":{}}